{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72c666a-529d-42a9-b104-73c9c6ae041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision.io import read_image\n",
    "import torcheval.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0b6590-adb6-4aec-8d7f-2ed91d60b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'modelfile':'wikiart.pth',\n",
    "    'trainingdir':'/home/guserbto@GU.GU.SE/wikiart/train',\n",
    "    'validationdir': '/home/guserbto@GU.GU.SE/wikiart/valid',\n",
    "    'testingdir': '/home/guserbto@GU.GU.SE/wikiart/test',\n",
    "    'device': 'cuda:2',\n",
    "    'epochs':20,\n",
    "    'batch_size':32,\n",
    "    \n",
    "}\n",
    "\n",
    "modelfile = args['modelfile']\n",
    "trainingdir = args['trainingdir']\n",
    "validationdir = args['validationdir']\n",
    "testingdir = args['testingdir']\n",
    "device = args['device']\n",
    "epochs = args['epochs']\n",
    "batch_size = args['batch_size']\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9128419-7a9f-4808-b12b-22257dec030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArtImage:\n",
    "    def __init__(self, imgdir, label, filename):\n",
    "        self.imgdir = imgdir\n",
    "        self.label = label\n",
    "        self.filename = filename\n",
    "        self.image = None\n",
    "        self.loaded = False\n",
    "\n",
    "    def get(self):\n",
    "        if not self.loaded:\n",
    "            self.image = read_image(os.path.join(self.imgdir, self.label, self.filename)).float()\n",
    "            self.loaded = True\n",
    "            self.image /= 255.0\n",
    "\n",
    "        return self.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238db86d-39f5-482a-9fa9-bf7f5f87165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArtDataset(Dataset):\n",
    "    def __init__(self, imgdir, device=\"cpu\"):\n",
    "        walking = os.walk(imgdir)\n",
    "        filedict = {}\n",
    "        indices = []\n",
    "        classes = set()\n",
    "        print(\"Gathering files for {}\".format(imgdir))\n",
    "        for item in walking:\n",
    "            sys.stdout.write('.')\n",
    "            arttype = os.path.basename(item[0])\n",
    "            artfiles = item[2]\n",
    "            for art in artfiles:\n",
    "                filedict[art] = WikiArtImage(imgdir, arttype, art)\n",
    "                indices.append(art)\n",
    "                classes.add(arttype)\n",
    "        print(\"...finished\")\n",
    "        self.filedict = filedict\n",
    "        self.imgdir = imgdir\n",
    "        self.indices = indices\n",
    "        self.classes = list(classes)\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filedict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgname = self.indices[idx]\n",
    "        imgobj = self.filedict[imgname]\n",
    "        ilabel = self.classes.index(imgobj.label)\n",
    "        image = imgobj.get().to(self.device)\n",
    "\n",
    "        return image, ilabel\n",
    "        \n",
    "    def get_image_by_filename(self):\n",
    "        if filename not in self.filedict:\n",
    "            raise ValueError(f\"Filename '{filename}' not found in the dataset.\")\n",
    "        \n",
    "        # Retrieve the image object and label\n",
    "        imgobj = self.filedict[filename]\n",
    "        ilabel = self.classes.index(imgobj.label)\n",
    "        image = imgobj.get().to(self.device)\n",
    "\n",
    "        return image, ilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3881dbfe-f09d-4c70-bac9-daa7b32ae0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArtModel(nn.Module):\n",
    "    def __init__(self, num_classes=27):\n",
    "        super(WikiArtModel, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(3, 32, kernel_size=4, padding=2)\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(kernel_size=2, padding=1)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(32, 64, kernel_size=4, padding=2)\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(kernel_size=2, padding=1)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.batchnorm1d = nn.BatchNorm1d(64 * 106 * 106)\n",
    "        self.linear1 = nn.Linear(64 * 106 * 106, 300)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(300, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        output = self.conv2d_1(image)\n",
    "        output = self.relu(output)\n",
    "        output = self.maxpool2d_1(output)\n",
    "\n",
    "        output = self.conv2d_2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.maxpool2d_2(output)\n",
    "\n",
    "        output = self.flatten(output)\n",
    "        output = self.batchnorm1d(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear2(output)\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac2d8d3-2916-45bd-a793-f4044f8eb542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering files for /home/guserbto@GU.GU.SE/wikiart/train\n",
      "...............................finished\n"
     ]
    }
   ],
   "source": [
    "# Prepping label weights for WeightedRandomSampler\n",
    "\n",
    "dataset = WikiArtDataset(trainingdir, device)\n",
    "\n",
    "labels = []\n",
    "for image, label in dataset:\n",
    "    labels.append(label)\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "label_weights_dict = {} \n",
    "for label, count in label_counts.items():\n",
    "    label_weights_dict[label] = 1/count\n",
    "\n",
    "weight_per_dataset_item = []\n",
    "for label in labels:\n",
    "    weight_per_dataset_item.append(label_weights_dict[label])\n",
    "\n",
    "\n",
    "weight_per_dataset_item = torch.tensor(weight_per_dataset_item, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a789c9c-8f24-4241-91ac-7dc7630b9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Random Sampler\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=weight_per_dataset_item, num_samples=len(weight_per_dataset_item), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad816a3-28f0-4259-b4f2-772ab6fcb2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train...\n",
      "Gathering files for /home/guserbto@GU.GU.SE/wikiart/train\n",
      "...............................finished\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "print(\"Time to train...\")\n",
    "\n",
    "traindataset = WikiArtDataset(trainingdir, device)\n",
    "\n",
    "def train(epochs=3, batch_size=32, modelfile=None, device=\"cpu\"):\n",
    "    train_loader = DataLoader(traindataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "    model = WikiArtModel().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Starting epoch {}\".format(epoch))\n",
    "        accumulate_loss = 0\n",
    "        for batch_id, batch in enumerate(tqdm.tqdm(train_loader)):\n",
    "            X, y = batch\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            accumulate_loss += loss\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"In epoch {}, loss = {}\".format(epoch, accumulate_loss))\n",
    "\n",
    "    if modelfile:\n",
    "        torch.save(model.state_dict(), modelfile)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657db5b8-e78a-4235-90dc-20bebc36290f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 824.00 MiB. GPU 2 has a total capacity of 10.90 GiB of which 161.31 MiB is free. Process 627257 has 8.38 GiB memory in use. Including non-PyTorch memory, this process has 2.37 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 155.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodelfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, modelfile, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, modelfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(traindataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m---> 10\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mWikiArtModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     12\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 824.00 MiB. GPU 2 has a total capacity of 10.90 GiB of which 161.31 MiB is free. Process 627257 has 8.38 GiB memory in use. Including non-PyTorch memory, this process has 2.37 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 155.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = train(args[\"epochs\"], args[\"batch_size\"], modelfile=args[\"modelfile\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675aa0a5-8f0f-45f5-8ff8-d3a79be2f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "print(\"Testing...\")\n",
    "\n",
    "testingdataset = WikiArtDataset(testingdir, device)\n",
    "\n",
    "def test(modelfile=None, device=\"cpu\"):\n",
    "    loader = DataLoader(testingdataset, batch_size=1)\n",
    "\n",
    "    model = WikiArtModel()\n",
    "    model.load_state_dict(torch.load(modelfile, weights_only=True))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    truth = []\n",
    "    for batch_id, batch in enumerate(tqdm.tqdm(loader)):\n",
    "        X, y = batch\n",
    "        y = y.to(device)\n",
    "        output = model(X)\n",
    "        predictions.append(torch.argmax(output).unsqueeze(dim=0))\n",
    "        truth.append(y)\n",
    "\n",
    "    predictions = torch.concat(predictions)\n",
    "    truth = torch.concat(truth)\n",
    "    metric = metrics.MulticlassAccuracy()\n",
    "    metric.update(predictions, truth)\n",
    "    print(\"Accuracy: {}\".format(metric.compute()))\n",
    "    confusion = metrics.MulticlassConfusionMatrix(27)\n",
    "    confusion.update(predictions, truth)\n",
    "    print(\"Confusion Matrix\\n{}\".format(confusion.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddd7c1-b991-4c0b-b62a-b0105947d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(modelfile=modelfile, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
